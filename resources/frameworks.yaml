---
#for doc purpose using <placeholder:default_value> syntax when it applies.

# FORMAT:
__dummy_framework_with_defaults:
  version: ''
  module: # defaults to `frameworks.framework_name`
  setup_args: ''
  params: {}
  project: http://url/to/project/repo
  image: # will result in built image `author/image:tag`
    author: automlbenchmark
    image:  # defaults to `framework name to lowercase`
    tag:  # defaults to `framework version`


#########################
### AutoML frameworks ###
#########################

AutoGluon:
  version: "latest"
  description: |
    AutoGluon-Tabular: Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection,
    AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers.
  project: https://auto.gluon.ai
  refs: [https://arxiv.org/abs/2003.06505]
#  params:
#    _save_artifacts: ['leaderboard', 'models', 'info']

# pretraining framework
FTT_pretrain_reconstruction:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000
        optimization.patience: 100000
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 30
      max_iter: 60000
      upload_per_n_iter: 30
      folder_name: "pretrain_reconstruction"


# finetuning
FTT_ft0:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_0/pretrained.ckpt"
      few_shot: 100
    holdout_frac: 0.125

FTT_ft250:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_250/pretrained.ckpt"
      few_shot: 100
    holdout_frac: 0.125

FTT_ft500:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_500/pretrained.ckpt"
      few_shot: 100
    holdout_frac: 0.125

FTT_ft750:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_750/pretrained.ckpt"
      few_shot: 100
    holdout_frac: 0.125

FTT_ft1000:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_1000/pretrained.ckpt"
      few_shot: 100
    holdout_frac: 0.125


