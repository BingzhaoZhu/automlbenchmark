---
#for doc purpose using <placeholder:default_value> syntax when it applies.

# FORMAT:
__dummy_framework_with_defaults:
  version: ''
  module: # defaults to `frameworks.framework_name`
  setup_args: ''
  params: {}
  project: http://url/to/project/repo
  image: # will result in built image `author/image:tag`
    author: automlbenchmark
    image:  # defaults to `framework name to lowercase`
    tag:  # defaults to `framework version`


#########################
### AutoML frameworks ###
#########################

AutoGluon:
  version: "latest"
  description: |
    AutoGluon-Tabular: Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection,
    AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers.
  project: https://auto.gluon.ai
  refs: [https://arxiv.org/abs/2003.06505]
#  params:
#    _save_artifacts: ['leaderboard', 'models', 'info']

##### pretraining framework #####
XTab_pretrain:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000  # never early stop until pretrain_kwargs.max_iter is reached

        pretrainer: true  # if true, use the custom Transformer implementation; otherwise use AutoGluon official implementation. Must be True.
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000 # never early stop until pretrain_kwargs.max_iter is reached
        optimization.patience: 100000  # never early stop until pretrain_kwargs.max_iter is reached

        finetune_on: "location-of-the-starting-checkpoint-on-s3" # to establish such a ckeckpoint, refer to ../hog_start.py
    pretrain_kwargs:
      is_pretrain: true # pretraining phase
      name: true # automatically assign name to checkpoints
      num_tasks: 52  # number of pretraining tasks
      iter_per_save: 1 # N=1 in federated learning
      max_iter: 2000 # maximum pretraining iterations
      upload_per_n_iter: 1 # how often to save the pretrained checkpoints
      bucket_name: "s3-bucket-name" # s3-bucket-name
      folder_name: "folder-name-in-s3-bucket" # folder-name-in-s3-bucket


XTab_finetune:
  extends: AutoGluon
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3   # early-stopping patience
        optimization.max_epochs: 3   # maximum epoch
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "backbone-checkpoint-saved-on-s3"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125
