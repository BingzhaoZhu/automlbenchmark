---
#for doc purpose using <placeholder:default_value> syntax when it applies.

# FORMAT:
__dummy_framework_with_defaults:
  version: ''
  module: # defaults to `frameworks.framework_name`
  setup_args: ''
  params: {}
  project: http://url/to/project/repo
  image: # will result in built image `author/image:tag`
    author: automlbenchmark
    image:  # defaults to `framework name to lowercase`
    tag:  # defaults to `framework version`


#########################
### AutoML frameworks ###
#########################

AutoGluon:
  version: "latest"
  description: |
    AutoGluon-Tabular: Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection,
    AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers.
  project: https://auto.gluon.ai
  refs: [https://arxiv.org/abs/2003.06505]
#  params:
#    _save_artifacts: ['leaderboard', 'models', 'info']

# pretraining framework
FTT_pretrain_reconstruction:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000
        optimization.patience: 100000
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_reconstruction_sum"


# heavy finetuning
FTT_ft0:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_0/pretrained.ckpt"
    holdout_frac: 0.125

FTT_ft15k:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128

        #optimization.patience: 20
        #optimization.top_k: 3
        #optimization.val_check_interval: 0.5

        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_15000/pretrained.ckpt"
    holdout_frac: 0.125

FTT_ft30k:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_30000/pretrained.ckpt"
    holdout_frac: 0.125

FTT_ft45k:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_45000/pretrained.ckpt"
    holdout_frac: 0.125

# FTT_ft1000:
#   extends: AutoGluon
#   version: "latest_gpu"
#   params:
#     hyperparameters:
#       FT_TRANSFORMER:
#         env.per_gpu_batch_size: 128
#         pretrainer: true
#         pretrainer.start_pretrain_coefficient: 0
#         pretrainer.end_pretrain_coefficient: 0
#         pretrainer.pretrain_epochs: 0
#     is_pretrain:
#       is_pretrain: false
#       finetune_on: "pretrain_reconstruction/iter_1000/pretrained.ckpt"
#     holdout_frac: 0.125


# light finetuning
FTT_ft0_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
        optimization.max_epochs: 3
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_0/pretrained.ckpt"
    holdout_frac: 0.125

FTT_ft15k_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128

        #optimization.patience: 20
        #optimization.top_k: 3
        #optimization.val_check_interval: 0.5

        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
        optimization.max_epochs: 3
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_15000/pretrained.ckpt"
    holdout_frac: 0.125

FTT_ft30k_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
        optimization.max_epochs: 3
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_30000/pretrained.ckpt"
    holdout_frac: 0.125

FTT_ft45k_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
        optimization.max_epochs: 3
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_45000/pretrained.ckpt"
    holdout_frac: 0.125


# fewshot
FTT_ft0_fewshot:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_0/pretrained.ckpt"
      few_shot: 100
    holdout_frac: 0.125

FTT_ft15k_fewshot:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128

        #optimization.patience: 20
        #optimization.top_k: 3
        #optimization.val_check_interval: 0.5

        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_15000/pretrained.ckpt"
      few_shot: 100
    holdout_frac: 0.125

FTT_ft30k_fewshot:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_30000/pretrained.ckpt"
      few_shot: 100
    holdout_frac: 0.125

FTT_ft45k_fewshot:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
    is_pretrain:
      is_pretrain: false
      finetune_on: "pretrain_reconstruction/iter_45000/pretrained.ckpt"
      few_shot: 100
    holdout_frac: 0.125

# FTT_ft1000_fewshot:
#   extends: AutoGluon
#   version: "latest_gpu"
#   params:
#     hyperparameters:
#       FT_TRANSFORMER:
#         env.per_gpu_batch_size: 128
#         pretrainer: true
#         pretrainer.start_pretrain_coefficient: 0
#         pretrainer.end_pretrain_coefficient: 0
#         pretrainer.pretrain_epochs: 0
#     is_pretrain:
#       is_pretrain: false
#       finetune_on: "pretrain_reconstruction/iter_1000/pretrained.ckpt"
#       few_shot: 100
#     holdout_frac: 0.125


