---
#for doc purpose using <placeholder:default_value> syntax when it applies.

# FORMAT:
__dummy_framework_with_defaults:
  version: ''
  module: # defaults to `frameworks.framework_name`
  setup_args: ''
  params: {}
  project: http://url/to/project/repo
  image: # will result in built image `author/image:tag`
    author: automlbenchmark
    image:  # defaults to `framework name to lowercase`
    tag:  # defaults to `framework version`


#########################
### AutoML frameworks ###
#########################

AutoGluon:
  version: "latest"
  description: |
    AutoGluon-Tabular: Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection,
    AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers.
  project: https://auto.gluon.ai
  refs: [https://arxiv.org/abs/2003.06505]
#  params:
#    _save_artifacts: ['leaderboard', 'models', 'info']

##### pretraining framework #####
FTT_pretrain_reconstruction:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000
        optimization.patience: 100000

        finetune_on: "pretrained_hogwild.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_reconstruction_sum"

FTT_pretrain_reconstruction_1:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000
        optimization.patience: 100000

        finetune_on: "pretrained_hogwild.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 1
      max_iter: 2000
      upload_per_n_iter: 1
      folder_name: "pretrain_reconstruction_1_sum_fold_2"

FTT_pretrain_reconstruction_5:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000
        optimization.patience: 100000

        finetune_on: "pretrained_hogwild.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_reconstruction_5_sum_fold_2"

FTT_pretrain_reconstruction_10:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000
        optimization.patience: 100000

        finetune_on: "pretrained_hogwild.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 10
      max_iter: 2000
      upload_per_n_iter: 10
      folder_name: "pretrain_reconstruction_10_sum"

FTT_pretrain_supervised:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.objective: None
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
        optimization.patience: 100000

        finetune_on: "pretrained_hogwild.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_supervised_sum"


FTT_pretrain_supervised_blk_1:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.objective: None
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
        optimization.patience: 100000

        model.fusion_transformer.n_blocks: 1
        finetune_on: "pretrained_hogwild_block_1.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_supervised_sum_block_1"

FTT_pretrain_supervised_blk_2:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.objective: None
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
        optimization.patience: 100000

        model.fusion_transformer.n_blocks: 2
        finetune_on: "pretrained_hogwild_block_2.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_supervised_sum_block_2"

FTT_pretrain_supervised_blk_3:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.objective: None
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
        optimization.patience: 100000

        model.fusion_transformer.n_blocks: 3
        finetune_on: "pretrained_hogwild.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_supervised_sum_block_3"

FTT_pretrain_supervised_with_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.objective: None
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
        optimization.patience: 100000

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 3
        finetune_on: "pretrained_hogwild_with_cls.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_supervised_sum_with_cls"

FTT_pretrain_supervised_only_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.objective: None
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
        optimization.patience: 100000

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 0
        finetune_on: "pretrained_hogwild_only_cls.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_supervised_sum_only_cls"

FTT_pretrain_contrastive:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "contrastive"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000
        optimization.patience: 100000

        finetune_on: "pretrained_hogwild.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_contrastive_sum"

FTT_pretrain_all:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 0
        optimization.patience: 100000

        finetune_on: "pretrained_hogwild.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_all_sum"


Fastformer_pretrain_reconstruction:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000
        optimization.patience: 100000

        model.fusion_transformer.additive_attention: true
        finetune_on: "pretrained_hogwild_Fastformer.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_reconstruction_fastformer"

Saint_pretrain_supervised:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.objective: None
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0
        optimization.patience: 100000

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrained_hogwild_Saint.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_supervised_saint"

Saint_pretrain_reconstruction:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000
        optimization.patience: 100000

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrained_hogwild_Saint.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_reconstruction_saint"

## fold 1
FTT_ft0_fold_1:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum/iter_0/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft250_fold_1:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128

        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum/iter_250/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft500_fold_1:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum/iter_500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft1000_fold_1:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum/iter_1000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft1500_fold_1:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum/iter_1500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft2000_fold_1:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum/iter_1990/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


##### light finetuning #####
FTT_ft0:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_0/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft250:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128

        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_250/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft500:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft1000:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_1000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft1500:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_1500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft2000:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_1990/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


##### light finetuning  #####
FTT_ft0_intense:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 20
        # optimization.max_epochs: 3
        optimization.top_k: 3
        optimization.val_check_interval: 0.5

        finetune_on: "pretrain_reconstruction_1_sum/iter_0/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft250_intense:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128

        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 20
        # optimization.max_epochs: 3
        optimization.top_k: 3
        optimization.val_check_interval: 0.5

        finetune_on: "pretrain_reconstruction_1_sum/iter_250/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft500_intense:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 20
        # optimization.max_epochs: 3
        optimization.top_k: 3
        optimization.val_check_interval: 0.5

        finetune_on: "pretrain_reconstruction_1_sum/iter_500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft1000_intense:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 20
        # optimization.max_epochs: 3
        optimization.top_k: 3
        optimization.val_check_interval: 0.5

        finetune_on: "pretrain_reconstruction_1_sum/iter_1000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft1500_intense:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 20
        # optimization.max_epochs: 3
        optimization.top_k: 3
        optimization.val_check_interval: 0.5

        finetune_on: "pretrain_reconstruction_1_sum/iter_1500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft2000_intense:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 20
        # optimization.max_epochs: 3
        optimization.top_k: 3
        optimization.val_check_interval: 0.5

        finetune_on: "pretrain_reconstruction_1_sum/iter_2000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125



##### fewshot ##### 
FTT_ft0_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_0/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft250_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128

        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_250/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft500_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft1000_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_1000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft1500_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_1500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft2000_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_1990/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


##### with cls #####
FTT_ft0_with_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 3
        finetune_on: "pretrain_supervised_sum_with_cls/iter_0/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft250_with_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128

        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 3
        finetune_on: "pretrain_supervised_sum_with_cls/iter_250/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft500_with_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 3
        finetune_on: "pretrain_supervised_sum_with_cls/iter_500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft1000_with_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 3
        finetune_on: "pretrain_supervised_sum_with_cls/iter_1000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft1500_with_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 3
        finetune_on: "pretrain_supervised_sum_with_cls/iter_1500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft2000_with_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 3
        finetune_on: "pretrain_supervised_sum_with_cls/iter_2000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

##### only cls #####
FTT_ft0_only_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 0
        finetune_on: "pretrain_supervised_sum_only_cls/iter_0/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft250_only_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128

        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 0
        finetune_on: "pretrain_supervised_sum_only_cls/iter_250/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft500_only_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 0
        finetune_on: "pretrain_supervised_sum_only_cls/iter_500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft1000_only_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 0
        finetune_on: "pretrain_supervised_sum_only_cls/iter_1000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft1500_only_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 0
        finetune_on: "pretrain_supervised_sum_only_cls/iter_1500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft2000_only_cls:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.share_qv_weights: true
        model.fusion_transformer.n_blocks: 0
        finetune_on: "pretrain_supervised_sum_only_cls/iter_2000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


##### heavy finetuning contrastive #####
FTT_ft0_cont:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_contrastive_sum/iter_0/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft250_cont:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128

        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_contrastive_sum/iter_250/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft500_cont:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_contrastive_sum/iter_500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft1000_cont:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_contrastive_sum/iter_1000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft1500_cont:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_contrastive_sum/iter_1500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


FTT_ft2000_cont:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_contrastive_sum/iter_2000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125



## Saint
Saint_ft0:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrain_reconstruction_saint/iter_0/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

Saint_ft250:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrain_reconstruction_saint/iter_250/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

Saint_ft500:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrain_reconstruction_saint/iter_500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

Saint_ft1000:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrain_reconstruction_saint/iter_1000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

Saint_ft1500:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrain_reconstruction_saint/iter_1500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

Saint_ft2000:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrain_reconstruction_saint/iter_2000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

##### light finetuning  #####
Saint_ft0_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrain_reconstruction_saint/iter_0/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

Saint_ft250_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrain_reconstruction_saint/iter_250/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

Saint_ft500_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrain_reconstruction_saint/iter_500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

Saint_ft1000_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrain_reconstruction_saint/iter_1000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

Saint_ft1500_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrain_reconstruction_saint/iter_1500/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

Saint_ft2000_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        model.fusion_transformer.row_attention: true
        model.fusion_transformer.row_attention_layer: "shared"
        # optimization.row_attention_weight_decay: 0.1
        finetune_on: "pretrain_reconstruction_saint/iter_2000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125


## 
FTT_pretrain_reconstruction_18_task:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000
        optimization.patience: 100000

        finetune_on: "pretrained_hogwild.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 18
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_reconstruction_mean_18_tasks"
      aggregation: "mean"

FTT_pretrain_reconstruction_1_task:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000
        optimization.patience: 100000

        finetune_on: "pretrained_hogwild.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 1
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_reconstruction_mean_1_tasks"
      aggregation: "mean"

FTT_pretrain_reconstruction_36_task:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000
        optimization.patience: 100000

        finetune_on: "pretrained_hogwild.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 36
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_reconstruction_mean_36_tasks"
      aggregation: "mean"

FTT_pretrain_reconstruction_52_task:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        optimization.max_epochs: 100000

        pretrainer: true
        pretrainer.augmentation_type: "permutation"
        pretrainer.corruption_rate: 0.6
        pretrainer.objective: "reconstruction"
        pretrainer.start_pretrain_coefficient: 1
        pretrainer.end_pretrain_coefficient: 1
        pretrainer.decay_pretrain_coefficient: 1
        pretrainer.pretrain_epochs: 100000
        optimization.patience: 100000

        finetune_on: "pretrained_hogwild.ckpt"
    is_pretrain:
      is_pretrain: true
      name: true
      num_tasks: 52
      iter_per_save: 5
      max_iter: 2000
      upload_per_n_iter: 5
      folder_name: "pretrain_reconstruction_mean_52_tasks"
      aggregation: "mean"


RF_AG:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      RF: {}
    holdout_frac: 0.125

GBM_AG:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      GBM: {}
    holdout_frac: 0.125

CAT_AG:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      CAT: {}
    holdout_frac: 0.125

XGB_AG:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      XGB: {}
    holdout_frac: 0.125

NN_AG:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      NN_TORCH: {}
    holdout_frac: 0.125

FASTAI_AG:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FASTAI: {}
    holdout_frac: 0.125


RF_HPO:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    HPO: true
    hyperparameters:
      RF: {}
    holdout_frac: 0.125

GBM_HPO:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    HPO: true
    hyperparameters:
      GBM: {}
    holdout_frac: 0.125

CAT_HPO:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    HPO: true
    hyperparameters:
      CAT: {}
    holdout_frac: 0.125

XGB_HPO:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    HPO: true
    hyperparameters:
      XGB: {}
    holdout_frac: 0.125

NN_HPO:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    HPO: true
    hyperparameters:
      NN_TORCH: {}
    holdout_frac: 0.125

FASTAI_HPO:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    HPO: true
    hyperparameters:
      FASTAI: {}
    holdout_frac: 0.125


FTT_ft0_fold_2_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_0/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft2000_fold_2_light:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        # optimization.patience: 3
        optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_2000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft0_fold_2_heavy:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_0/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125

FTT_ft2000_fold_2_heavy:
  extends: AutoGluon
  version: "latest_gpu"
  params:
    hyperparameters:
      FT_TRANSFORMER:
        env.per_gpu_batch_size: 128
        pretrainer: true
        pretrainer.start_pretrain_coefficient: 0
        pretrainer.end_pretrain_coefficient: 0
        pretrainer.pretrain_epochs: 0

        optimization.patience: 3
        # optimization.max_epochs: 3
        optimization.top_k: 1
        optimization.val_check_interval: 1.0

        finetune_on: "pretrain_reconstruction_1_sum_fold_2/iter_2000/pretrained.ckpt"
    is_pretrain:
      is_pretrain: false
    holdout_frac: 0.125
